{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Perceptrons and XOR\n",
    "\n",
    "Perceptrons, part of work in cybernetics, showed early promise but failed to produce a learning algorithm capable of learning the XOR function. This is because XOR is not *linearly separable*.\n",
    "\n",
    "We demonstrate these ideas using linear and non-linear models built using TensorFlow/Keras rather than using the original perceptron formulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following class will implement a variety of simple MLP models for logic gates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class LogicGateModel:\n",
    "    '''This class models a two-input logic gate function, i.e., AND, OR, XOR, NAND, NOR, XNOR\n",
    "    hidden_layers gives a list of layer specifications of the form: (#ofNodes, activationFunction);\n",
    "    output_layer is similar but with only layer specification.\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 optimizer=SGD(lr=0.1),\n",
    "                 hidden_layers=[],\n",
    "                 output_layer=(1, tf.keras.activations.linear),\n",
    "                 loss_function=tf.keras.losses.mse\n",
    "                 ):\n",
    "        self.model = Sequential()\n",
    "        for layer in hidden_layers:\n",
    "            self.model.add(Dense(layer[0], input_dim=2, activation=layer[1]))\n",
    "        self.model.add(Dense(output_layer[0], input_dim=2, activation=output_layer[1]))\n",
    "        self.model.compile(loss=loss_function,\n",
    "                           optimizer=optimizer,\n",
    "                           metrics=[tf.keras.metrics.binary_accuracy]\n",
    "                           )\n",
    "\n",
    "    def train(self, X, y, n_epochs=100):\n",
    "        self.model.fit(X, y, batch_size=1, epochs=n_epochs, verbose=0)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define AND, OR and XOR logic functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Correct answers for the three logical functions:\n",
      "\t AND\tOR\tXOR\n",
      "[0 0] ->  0\t 0\t 0\n",
      "[0 1] ->  0\t 1\t 1\n",
      "[1 0] ->  0\t 1\t 1\n",
      "[1 1] ->  1\t 1\t 0\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y_and = np.array([[0],[0],[0],[1]])\n",
    "y_or = np.array([[0],[1],[1],[1]])\n",
    "y_xor = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "print('Correct answers for the three logical functions:')\n",
    "print('\\t AND\\tOR\\tXOR')\n",
    "for i in range(4):\n",
    "    print(f'{X[i]} -> ',\n",
    "          f'{X[i,0] & X[i,1]}\\t',\n",
    "          f'{X[i,0] | X[i,1]}\\t',\n",
    "          f'{X[i,0] ^ X[i,1]}'\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AND function can be learned by a one-layer network with a linear activation function. See how the output for [1,1] is clearly the highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[[-0.21683198]\n",
      " [ 0.3096367 ]\n",
      " [ 0.3371088 ]\n",
      " [ 0.86357754]]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "model = LogicGateModel(output_layer=(1, tf.keras.activations.linear))\n",
    "model.train(X, y_and)\n",
    "print(model.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same holds for the OR function. See how the output value for [0.0] is clearly lower than the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[[0.28801927]\n",
      " [0.7916323 ]\n",
      " [0.7597754 ]\n",
      " [1.2633884 ]]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "model = LogicGateModel(output_layer=(1, tf.keras.activations.linear))\n",
    "model.train(X, y_or)\n",
    "print(model.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, a linear model cannot learn the XOR function, regardless of how many layers we add. \n",
    "See how none of the outputs fail to converge on the correct solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[[0.5376339 ]\n",
      " [0.53214777]\n",
      " [0.4777659 ]\n",
      " [0.47227973]]\n",
      "[[0.5545918 ]\n",
      " [0.5560949 ]\n",
      " [0.55774784]\n",
      " [0.559251  ]]\n",
      "[[0.5256991 ]\n",
      " [0.5243015 ]\n",
      " [0.52510256]\n",
      " [0.52370495]]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# One layer\n",
    "model = LogicGateModel(output_layer=(1, tf.keras.activations.linear))\n",
    "model.train(X, y_xor)\n",
    "print(model.predict(X))\n",
    "\n",
    "# Two layers\n",
    "model = LogicGateModel(hidden_layers=[(2, tf.keras.activations.linear)])\n",
    "model.train(X, y_xor)\n",
    "print(model.predict(X))\n",
    "\n",
    "# More layers\n",
    "model = LogicGateModel(hidden_layers=[\n",
    "    (4, tf.keras.activations.linear),\n",
    "    (2, tf.keras.activations.linear)\n",
    "])\n",
    "model.train(X, y_xor)\n",
    "print(model.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A non-linear regression model with a two layer network can learn the XOR function. \n",
    "Here, a non-linear tanh() is used. \n",
    "This model, suggested by Goodfellow, et al., does not always find the solution.\n",
    "Here, I ran this code a few times until it found one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[[0.       ]\n",
      " [0.9999998]\n",
      " [0.9999998]\n",
      " [0.       ]]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "model = LogicGateModel(\n",
    "    hidden_layers=[(2, tf.nn.tanh)],\n",
    "    output_layer=(1, tf.keras.activations.linear)\n",
    ")\n",
    "model.train(X, y_xor, n_epochs=500)\n",
    "print(model.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This slightly more complicated model, suggested by S. Park, works more reliably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[[0.0014103 ]\n",
      " [0.9952068 ]\n",
      " [0.99467033]\n",
      " [0.00644412]]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "model = LogicGateModel(\n",
    "    hidden_layers=[(8, tf.nn.tanh)],\n",
    "    output_layer=(1, tf.nn.sigmoid),\n",
    "    loss_function=tf.keras.losses.binary_crossentropy\n",
    ")\n",
    "model.train(X, y_xor, n_epochs=1000)\n",
    "print(model.predict(X))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}